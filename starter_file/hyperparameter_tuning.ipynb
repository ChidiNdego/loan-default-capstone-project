{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning using HyperDrive\n",
    "\n",
    "Import Dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531914256
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import joblib"
   ]
  },
  {
   "source": [
    "### Load Workspace\n",
    "\n",
    "Load the exisiting azure workspace. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531917374
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "experiment_name = 'loan_default_model'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "run = experiment.start_logging()"
   ]
  },
  {
   "source": [
    "### Compute Cluster Configuration\n",
    "\n",
    "Setting up the virtual machine and other cluster configuration needed in running the experiment. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cluster_name = \"aml-cluster\" \n",
    "\n",
    "#verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print(f\"Found existing cluster: {cpu_cluster_name} to be used.\")\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2', max_nodes=6)\n",
    "\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598531923519
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Hyperdrive Configuration\n",
    "\n",
    "The project entails identifying customers who would either default or not after taking a loan credit. This transalates to a binary classification: to default or not default. Hence, a Logistic regression model would be used. SKLearn's logistic regression algorithm is a well-known supervised learning approach optimized for dichotomous or binary variables.\n",
    "\n",
    "Hyperparameters are adjustable parameters that let you control the model training process. Hyperparameter tuning is the process of finding the configuration of hyperparameters that results in the best performance. The process is typically computationally expensive and manual.\n",
    "The two hyperparamters used in this experiment are `C` and `max_iter`. `C` is the Inverse Regularization Strength which applies a penalty to stop increasing the magnitude of parameter values in order to reduce overfitting. `max_iter` is the maximum iteration to converge for the SKLearn Logistic Regression algorithm.\n",
    "\n",
    "Primary Metric is used to optimize the hyperparamter tuning. Each training run is evaluated for the primary metric.\n",
    "We have chosen `Accuracy` as the primary metric. `MAXIMIZE` is the preferred primary metric goal.\n",
    "\n",
    "Early termination policies are applied to HyperDrive runs. A run is cancelled when the criteria of a specified policy are met. The `BanditPolicy` was used.\n",
    "It is based on slack factor/slack amount and evaluation interval. Bandit terminate runs where the primary metric is not within the specified slack factor/slack amount compared to the best performing run. This helps to improves computational efficiency.\n",
    "\n",
    "For this experiment, `evaluation_interval=1`, `slack_factor=0.2`, and `delay_evaluation=4`. This configration means that the policy would be applied to every even number iteration of the pipeline greater than 4 and if 1.2*value of the benchmark metric for current iteration is smaller than the best metric value so far, the run will be cancelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544893076
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "early_termination_policy = BanditPolicy(evaluation_interval=1, slack_factor=0.2, delay_evaluation=4)\n",
    "\n",
    "param_sampling = RandomParameterSampling(parameter_space={\"--C\": choice(1, 2, 3, 4, 5, 6), \"--max_iter\": choice(50, 100, 150, 200, 250, 300)})\n",
    "\n",
    "# Code below creates a new training directory with the train.py script in it\n",
    "if \"training\" not in os.listdir():\n",
    "    os.mkdir(\"./training\")\n",
    "\n",
    "shutil.copy('train.py', \"./training\")\n",
    "\n",
    "# Code creates an output directory to store trained models\n",
    "if \"outputs\" not in os.listdir():\n",
    "    os.makedirs('./outputs', exist_ok=True)\n",
    "\n",
    "estimator =  SKLearn(source_directory='./training', entry_script=\"train.py\", compute_target=cpu_cluster)\n",
    "\n",
    "hyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n",
    "                                     hyperparameter_sampling=param_sampling,\n",
    "                                     policy=early_termination_policy,\n",
    "                                     primary_metric_name='Accuracy',\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                     max_total_runs=20,\n",
    "                                     max_concurrent_runs=4)"
   ]
  },
  {
   "source": [
    "### Submitting the experiment"
   ],
   "cell_type": "markdown",
   "metadata": {
    "gather": {
     "logged": 1598544897941
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Submitting Hyperdrive experiment...')\n",
    "hyperdrive_run = experiment.submit(hyperdrive_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598544898497
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "Use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546648408
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "RunDetails(hyperdrive_run).show()\n",
    "hyperdrive_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "Get the best model from the hyperdrive experiments and display all the properties of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546650307
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "\n",
    "print('Best Run Details:', best_run,\n",
    "      'Best Run ID:', best_run.id, \n",
    "      'Arguments:', best_run.get_details()['runDefinition']['arguments'],\n",
    "      'Accuracy:', best_run_metrics['Accuracy'],\n",
    "      'Regularization Strength (C): ', best_run_metrics['Regularization Strength:'],\n",
    "      'Maximum Iterations (max_iter): ', best_run_metrics['Max iterations:'], sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546657829
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "best_model = best_run.register_model(\n",
    "    model_name='HypDriveBestModel', \n",
    "    model_path='outputs/', \n",
    "    properties={'Accuracy': best_run_metrics['Accuracy'], \n",
    "                'Regularization Strength (C)': best_run_metrics['Regularization Strength:'], \n",
    "                'Maximum Iterations (max_iter)': best_run_metrics['Max iterations:']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "1e915f0a29dc84041eaeb02b7b1a21c440e37a87b61d44d5e84a515737dc82bc"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}